# Locust on AWS EKS - Complete SRE Deployment Guide

## üìã Overview

This project provides a **production-ready, Infrastructure-as-Code (IaC) solution** for deploying distributed Locust load testing on AWS EKS (Elastic Kubernetes Service).

### What This Does

Automates the complete deployment of:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           AWS Cloud Environment              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ        EKS Kubernetes Cluster            ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îÇ Locust Master    ‚îÇ  (1 replica)      ‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îÇ - Coordinator    ‚îÇ  Web UI: port 8089‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îÇ - Web Dashboard  ‚îÇ  Master: port 5557‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ ‚îÇ
‚îÇ ‚îÇ         ‚îÇ Commands                       ‚îÇ ‚îÇ
‚îÇ ‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ         ‚ñº         ‚ñº          ‚ñº          ‚ñº ‚îÇ
‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê...  ‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îÇ Worker 1‚îÇ‚îÇ Worker 2‚îÇ‚îÇ Worker 3‚îÇ      ‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îÇ (Load)  ‚îÇ‚îÇ (Load)  ‚îÇ‚îÇ (Load)  ‚îÇ      ‚îÇ ‚îÇ
‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ
‚îÇ ‚îÇ         ‚îÇ    Generates Load              ‚îÇ ‚îÇ
‚îÇ ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ                      ‚Üì                    ‚îÇ ‚îÇ
‚îÇ ‚îÇ            Target API (Any Public API)   ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ ‚îÇ  Auto-scales: 3-20 workers based on CPU ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Features

‚úÖ **Complete IaC**: Everything defined in code (Terraform + Kubernetes YAML)
‚úÖ **Single Command Deployment**: `./deploy.sh` for everything
‚úÖ **Single Command Destruction**: `./destroy.sh` for safe cleanup
‚úÖ **Auto-Scaling**: Kubernetes HPA scales workers 3-20 based on load
‚úÖ **Production-Ready**: Security best practices, monitoring, logging
‚úÖ **Multi-Environment**: Dev, staging, production configurations
‚úÖ **Well-Documented**: Comprehensive guides and inline comments
‚úÖ **Cost-Conscious**: Easy cleanup prevents unexpected bills
‚úÖ **Prometheus Integration**: Full metrics collection with dashboard-compatible naming
‚úÖ **Persistent Access**: Automatic port-forwards with health checks (survive pod restarts)
‚úÖ **Grafana Dashboards**: Pre-configured monitoring dashboards
‚úÖ **Zero-Maintenance Monitoring**: Automatic recovery if services go down

---

## üöÄ Quick Start

### Prerequisites

Ensure you have installed:
- `terraform` >= 1.0
- `aws-cli` >= 2.0
- `kubectl` >= 1.28
- `docker` >= 20.0
- `jq` (JSON parser)
- AWS account with IAM permissions

### Deploy in 3 Steps

```bash
# 1. Navigate to project
cd /home/lostborion/Documents/veeam-extended

# 2. Deploy (takes 30-40 minutes)
./deploy.sh

# 3. Access Locust UI
# URL will be displayed at the end
```

That's it! Your Locust cluster is running on AWS EKS.

### Full Deployment Output

The script will:
1. ‚úì Validate prerequisites
2. ‚úì Create AWS infrastructure (VPC, EKS, ECR) ~20min
3. ‚úì Configure kubectl to access cluster
4. ‚úì Build Docker image with Locust
5. ‚úì Push image to ECR
6. ‚úì Deploy to Kubernetes ~3min
7. ‚úì Display LoadBalancer URL to access web UI

---

## üìÅ Project Structure

```
veeam-extended/
‚îÇ
‚îú‚îÄ‚îÄ deploy.sh                           # ‚≠ê Single-command deployment
‚îú‚îÄ‚îÄ destroy.sh                          # ‚≠ê Single-command destruction
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ terraform/                          # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                         # AWS resources (VPC, EKS, ECR)
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf                    # Configuration variables
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf                      # Output values
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars                # Default values
‚îÇ
‚îú‚îÄ‚îÄ docker/                             # Container definition
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                      # Multi-stage build
‚îÇ   ‚îú‚îÄ‚îÄ entrypoint.sh                   # Container startup script
‚îÇ   ‚îî‚îÄ‚îÄ .dockerignore
‚îÇ
‚îú‚îÄ‚îÄ kubernetes/                         # K8s manifests
‚îÇ   ‚îî‚îÄ‚îÄ base/
‚îÇ       ‚îú‚îÄ‚îÄ namespace.yaml              # Isolated environment
‚îÇ       ‚îú‚îÄ‚îÄ configmap.yaml              # Test configuration
‚îÇ       ‚îú‚îÄ‚îÄ master-deployment.yaml      # Locust coordinator
‚îÇ       ‚îú‚îÄ‚îÄ master-service.yaml         # LoadBalancer service
‚îÇ       ‚îú‚îÄ‚îÄ worker-deployment.yaml      # Load generators
‚îÇ       ‚îî‚îÄ‚îÄ worker-hpa.yaml             # Auto-scaling config
‚îÇ
‚îú‚îÄ‚îÄ tests/                              # Load test scenarios
‚îÇ   ‚îú‚îÄ‚îÄ locustfile.py                   # Test entry point
‚îÇ   ‚îú‚îÄ‚îÄ scenarios/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jsonplaceholder.py          # JSONPlaceholder API tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ httpbin.py                  # HTTPBin tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ custom.py                   # Template for custom tests
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/                            # Automation
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ colors.sh                   # Colored output functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.sh                   # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ deploy/                         # Deployment phases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01-validate-prereqs.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02-deploy-infrastructure.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03-configure-kubectl.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04-build-push-image.sh
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 05-deploy-kubernetes.sh
‚îÇ   ‚îú‚îÄ‚îÄ destroy/                        # Destruction phases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01-delete-k8s-resources.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02-delete-ecr-images.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03-destroy-infrastructure.sh
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 04-cleanup-local.sh
‚îÇ   ‚îî‚îÄ‚îÄ utils/                          # Utility scripts (monitoring, scaling, etc)
‚îÇ
‚îú‚îÄ‚îÄ config/                             # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ environments/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prod/
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars
‚îÇ
‚îú‚îÄ‚îÄ docs/                               # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ TERRAFORM_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îî‚îÄ‚îÄ QUICKSTART.md
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml                      # Python dependencies
‚îú‚îÄ‚îÄ poetry.lock                         # Locked versions
‚îî‚îÄ‚îÄ reports/                            # Test results (gitignored)
```

---

## üíª Usage

### Deploy to Dev Environment

```bash
./deploy.sh dev
```

### Deploy to Production with Custom Tag

```bash
./deploy.sh prod v1.2.3
```

### Destroy All Resources

```bash
./destroy.sh
# You'll be asked to confirm with 'destroy' and 'yes'
```

### Access All Services

All services have **persistent, auto-recovering port-forwards** that survive pod restarts and system reboots:

#### üîµ Locust Web UI (Load Testing)
```
http://localhost:8089
```
- Dashboard for running tests
- Real-time metrics
- User and spawn rate controls

#### üìä Locust Metrics (Prometheus Format)
```
http://localhost:9091/metrics
```
- Raw Prometheus metrics
- Scraped by Prometheus every 30 seconds
- Includes: requests, response times, failures, percentiles, etc.

#### üìà Grafana (Dashboards)
```
http://localhost:3000
Username: admin
Password: admin123
```
- Pre-configured Locust dashboards
- Request rates, response times, error rates
- User and worker count metrics

#### üìâ Prometheus (Metrics Database)
```
http://localhost:9090
```
- Query Prometheus metrics directly
- PromQL support
- Data retention: 7-30 days (configurable)

**All port-forwards are automatic and persistent:**
- ‚úÖ Start automatically on system boot
- ‚úÖ Auto-recover if they fail (health check every 1 minute)
- ‚úÖ Survive pod redeployments
- ‚úÖ No terminal session needed
- ‚úÖ Available 24/7

### Common Operations

**View pod status:**
```bash
kubectl get pods -n locust
```

**View logs (master):**
```bash
kubectl logs deployment/locust-master -n locust -f
```

**View logs (workers):**
```bash
kubectl logs deployment/locust-worker -n locust -f
```

**Scale workers manually:**
```bash
kubectl scale deployment locust-worker --replicas=10 -n locust
```

**Monitor auto-scaling:**
```bash
kubectl get hpa -n locust -w
```

---

## üèóÔ∏è Architecture

### Components

**Master Node** (1 replica)
- Coordinates all workers
- Serves web dashboard (port 8089)
- Listens for worker connections (port 5557)
- Aggregates test results

**Worker Nodes** (3-20 replicas, auto-scaled)
- Run actual load tests
- Execute Locust test scenarios
- Send metrics to master
- Auto-scale based on CPU/memory usage

**Load Balancer Service**
- Exposes master web UI to external access
- AWS LoadBalancer (hostname/IP auto-assigned)

**Horizontal Pod Autoscaler (HPA)**
- Scales workers automatically
- Trigger: CPU > 70% or Memory > 80%
- Min replicas: 3, Max replicas: 20

### Infrastructure

**AWS Resources Created:**
- VPC with public and private subnets
- EKS Cluster (managed Kubernetes)
- Auto-scaling node group (t3.medium instances)
- EC2 NAT Gateways (for secure egress)
- ECR Repository (private Docker registry)
- CloudWatch Logs (for auditing and debugging)
- IAM roles and security groups

**Region:** eu-central-1 (Frankfurt)

---

## üìä Cost Estimation

| Component | Cost/Hour | Cost/Day | Cost/Month |
|-----------|-----------|----------|-----------|
| EKS Control Plane | $0.10 | $2.40 | $73 |
| 3x t3.medium nodes | $0.125 | $3.00 | $90 |
| 2x NAT Gateways | $0.09 | $2.16 | $65 |
| LoadBalancer | $0.023 | $0.55 | $16 |
| CloudWatch Logs | $0.007 | $0.17 | $5 |
| **TOTAL** | **$0.34** | **$8.28** | **$249** |

### Cost Optimization

**Option 1: Use Spot Instances (saves 60-70%)**
```bash
# Edit terraform/terraform.tfvars
capacity_type = "SPOT"
```

**Option 2: Single NAT Gateway (saves $33/month)**
```bash
# Edit terraform/main.tf
single_nat_gateway = true
```

**Option 3: NodePort Service (saves $16/month)**
Replace LoadBalancer with NodePort in kubernetes/base/master-service.yaml

**Recommendation:** Delete cluster when not in use
```bash
./destroy.sh  # Takes ~15 minutes, saves all future costs
```

---

## üîê Security

This solution implements:

‚úì **Private subnets** for worker nodes (no public IPs)
‚úì **Security groups** with least-privilege access
‚úì **Non-root containers** (UID 1000)
‚úì **Multi-stage Docker builds** (smaller attack surface)
‚úì **IAM least-privilege roles** (only required permissions)
‚úì **CloudWatch audit logging** (track all changes)
‚úì **Encrypted communication** (HTTPS/TLS)

### Additional Hardening (Future)

- AWS Secrets Manager for credentials
- Pod Network Policies (restrict inter-pod traffic)
- RBAC (Role-Based Access Control)
- Private EKS endpoint
- VPC Flow Logs

---

## üìö Documentation

- **[QUICKSTART.md](docs/QUICKSTART.md)** - Quick reference for common tasks
- **[DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md)** - Complete deployment walkthrough
- **[TERRAFORM_GUIDE.md](docs/TERRAFORM_GUIDE.md)** - Terraform-specific details
- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System architecture and design
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - Common issues and solutions

---

## üß™ Testing

The project includes multiple test scenarios:

### JSONPlaceholder (Default)
Tests against free public API (https://jsonplaceholder.typicode.com)

- `GET /posts` - List posts
- `GET /posts/{id}` - Get specific post
- `POST /posts` - Create post
- `PUT /posts/{id}` - Update post
- `DELETE /posts/{id}` - Delete post

### HTTPBin
Tests HTTP functionality (https://httpbin.org)

- GET/POST/PUT/DELETE requests
- Headers and authentication
- Data compression
- Cookies and redirects

### Custom Tests
Template for adding your own API tests:

```python
# tests/scenarios/my_api.py
from locust import HttpUser, task, between

class MyAPIUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def my_task(self):
        self.client.get("/api/endpoint")
```

Then set environment variable:
```bash
export SCENARIO=my_api
./deploy.sh
```

---

## üîÑ Lifecycle

### Initial Deployment

```bash
./deploy.sh dev          # Deploy development environment
# ~40 minutes total
```

### Running Tests

1. Access Locust UI: `http://<LoadBalancer-IP>:8089`
2. Click "New Test Run"
3. Configure:
   - Number of users
   - Spawn rate
   - Run time
4. Click "Start"
5. Monitor results in dashboard

### Monitoring

**In Locust UI:**
- Real-time request/response metrics
- Success/failure rates
- Response time percentiles
- Charts and graphs

**Via kubectl:**
```bash
kubectl top pods -n locust      # CPU/Memory usage
kubectl get hpa -n locust       # Auto-scaler metrics
kubectl logs -f ...             # Live logs
```

**Via AWS Console:**
- CloudWatch logs
- EKS cluster metrics
- EC2 instance metrics

### Cleanup

```bash
./destroy.sh             # Destroy all resources
# ~15-20 minutes
```

---

## üêõ Troubleshooting

### Deployment Hangs
- Check logs: `kubectl logs deployment/locust-master -n locust`
- Verify node status: `kubectl get nodes`
- Check auto-scaling: `kubectl get hpa -n locust`

### Can't Access Locust UI
- LoadBalancer IP may take 2-3 minutes to assign
- Check: `kubectl get svc locust-master -n locust`
- Use port-forward: `kubectl port-forward -n locust svc/locust-master 8089:8089`

### Workers Won't Connect
- Check master logs: `kubectl logs deployment/locust-master -n locust`
- Verify service is running: `kubectl get svc locust-master -n locust`
- Check pod network: `kubectl get pods -n locust -o wide`

### Out of Memory
- Increase memory limits in kubernetes/base/worker-deployment.yaml
- Reduce number of users per pod
- Increase number of worker replicas

### Costs Higher Than Expected
- Delete/destroy cluster when not in use
- Check EKS CloudWatch logs for errors
- Review AWS billing dashboard

See [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for more help.

---

## ü§ù Contributing

This is an SRE-focused project. Contributions welcome for:
- Terraform module improvements
- Additional test scenarios
- Kubernetes optimizations
- Documentation updates
- Bug fixes

---

## üìÑ License

This project is provided as-is for educational and development purposes.

---

## üìû Support

For issues:
1. Check [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)
2. Review logs: `kubectl logs -f deployment/locust-master -n locust`
3. Check AWS console for infrastructure issues
4. Verify AWS credentials: `aws sts get-caller-identity`

---

## üéØ Next Steps

1. **Review Architecture**: Read [ARCHITECTURE.md](docs/ARCHITECTURE.md)
2. **Deploy**: Run `./deploy.sh dev`
3. **Run Test**: Access UI and start load test
4. **Monitor**: Watch metrics in dashboard
5. **Iterate**: Adjust load parameters and run again
6. **Cleanup**: Run `./destroy.sh`

Happy load testing! üöÄ
